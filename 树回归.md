# 树回归

#### 主要包括：了解CART算法的实现和树的剪枝方法 
## ID3算法的弊端

决策树的树构建算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切分成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。

除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征离散化，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在特性。
## CART算法
与ID3算法相反，CART算法正好适用于连续型特征。CART算法使用二元切分法来处理连续型变量。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体的处理方法是：如果特征值大于给定值就走左子树，否则就走右子树

CART算法有两步：
+ 决策树生成：递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义"最好"：
+ 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准


CART算法：

假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集：
![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_2.png)

其中，D表示整个数据集合，n为特征数。

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元R1,R2,...Rm，并且在每个单元Rm上有一个固定的输出值Cm，于是回归树模型可表示为：

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_3.png)
          
这样就可以计算模型输出值与实际值的误差：

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_4.png)

我们希望每个单元上的Cm，可以是的这个平方误差最小化。易知，当Cm为相应单元的所有实际值的均值时，可以到最优：

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_5.png)

为了生成这些单元划分
假设选择变量 xj 为切分变量，它的取值 s 为切分点，那么就会得到两个区域：
![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_6.png)

当j和s固定时，我们要找到两个区域的代表值c1，c2使各自区间上的平方差最小：

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_7.png)

前面已经知道c1，c2为区间上的平均

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_8.png)

那么对固定的 j 只需要找到最优的s，然后通过遍历所有的变量，我们可以找到最优的j，这样我们就可以得到最优对（j，s），并得到两个区间。

这样的回归树通常称为最小二乘回归树（least squares regression tree）。

上述过程表示的算法步骤为：

![](https://cuijiahua.com/wp-content/uploads/2017/12/ml_13_9.png)

除此之外，我们再定义两个参数，tolS和tolN，分别用于控制误差变化限制和切分特征最少样本数。这两个参数的意义是什么呢？就是防止过拟合，提前设置终止条件，实际上是在进行一种所谓的预剪枝（prepruning）操作

## 树剪枝


一棵树如果结点过多，表明该模型可能对数据进行了“过拟合”。

通过降低树的复杂度来避免过拟合的过程称为剪枝（pruning）。上小节我们也已经提到，设置tolS和tolN就是一种预剪枝操作。另一种形式的剪枝需要使用测试集和训练集，称作后剪枝（postpruning）。

## 总结

+ CART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。
+ 一颗过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝和后剪枝，预剪枝更有效但需要用户定义一些参数。